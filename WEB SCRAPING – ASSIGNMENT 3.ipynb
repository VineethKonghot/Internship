{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7254f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3a8ea",
   "metadata": {},
   "source": [
    "# 1.\n",
    "Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f28eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "driver.get('https://www.amazon.in')\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39eec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the text to be searched: slr cameras\n"
     ]
    }
   ],
   "source": [
    "user_input=input(\"Enter the text to be searched: \")\n",
    "user_input\n",
    "search_job=driver.find_element_by_xpath(\"//input[@id='twotabsearchtextbox']\")\n",
    "search_job.send_keys(user_input)\n",
    "click_search=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "click_search.click()\n",
    "#slr cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddf74f",
   "metadata": {},
   "source": [
    "# 2.\n",
    "In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86114f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_urls = []\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 3 page\n",
    "    url1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-no-outline']\")       #scraping urls\n",
    "    for i in url1:\n",
    "        product_urls.append(i.get_attribute(\"href\"))                       #appending the urls in product_urls list\n",
    "        \n",
    "    next_page=driver.find_element_by_xpath(\"//span[@class='s-pagination-strip']/a[3]\").get_attribute('href')\n",
    "    driver.get(next_page)   \n",
    "    driver.refresh() \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "prod = []\n",
    "rate = []\n",
    "price = []\n",
    "delv = []\n",
    "avl = []\n",
    "det = []\n",
    "product_url=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in product_urls:\n",
    "    driver.get(j)\n",
    "    product_urls.append(j)\n",
    "    driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        brandtag = driver.find_element_by_xpath(\" //div[@class='a-section a-spacing-small a-spacing-top-small']/table/tbody/tr[2]/td[2]/span\")\n",
    "        Brand.append(brandtag.text)\n",
    "    except:\n",
    "        Brand.append('-')\n",
    "    \n",
    "    try:\n",
    "        prodtag = driver.find_element_by_xpath(\"//span[@class='a-size-large product-title-word-break']\")\n",
    "        prod.append(prodtag.text)\n",
    "    except:\n",
    "        prod.append('-')\n",
    "        \n",
    "    try:\n",
    "        ratetag = driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']\")\n",
    "        rate.append(ratetag.get_attribute(\"innerHTML\"))\n",
    "    except NoSuchElementException:\n",
    "        rate.append('-')\n",
    "    \n",
    "    try:\n",
    "        pricetag = driver.find_element_by_xpath(\"//div[@id='corePrice_feature_div']\")\n",
    "        price.append(pricetag.text)\n",
    "    except:\n",
    "        price.append('-')\n",
    "    \n",
    "    try:\n",
    "        delvtag = driver.find_element_by_xpath(\"//div[@id='deliveryBlockMessage']/div/div/b\")\n",
    "        delv.append(delvtag.text)\n",
    "    except:\n",
    "        delv.append('-')\n",
    "    try:\n",
    "        avltag = driver.find_element_by_xpath(\"//div[@id='availability']//span\")\n",
    "        avl.append(avltag.text)\n",
    "    except:\n",
    "        avl.append('-')\n",
    "    try:\n",
    "        dettag = driver.find_element_by_xpath(\"//div[@id='feature-bullets']\")\n",
    "        det.append(dettag.text.replace('\\nShow More','.').replace('\\n',''))\n",
    "    except:\n",
    "        det.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad79ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.DataFrame({})\n",
    "amazon['Brand Name'] = Brand[:10]\n",
    "amazon['Product Name']=prod[:10]\n",
    "amazon['Rating']=rate[:10]\n",
    "amazon['Price']=price[:10]\n",
    "amazon['Expected Delivery']=delv[:10]\n",
    "amazon['Availablitiy']=avl[:10]\n",
    "amazon['Details']=det[:10]\n",
    "\n",
    "amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon.to_csv('products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d084900",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254d249",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "411e9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://images.google.com/')\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a54d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Define a function which accepts image name as an argument\n",
    "def get_urls(keywords):\n",
    "        driver.get(url)\n",
    "        \n",
    "#Let's enter the details in the search column and click search button\n",
    "        search_bar=driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\")\n",
    "        search_bar.send_keys(keywords)\n",
    "        button=driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\")\n",
    "        button.click()\n",
    "        driver.implicitly_wait(5)\n",
    "        \n",
    "        \n",
    "#Lets's load few pages to scrape images from them\n",
    "        for k in range(12):\n",
    "            driver.find_element_by_xpath(\"//a[@class='VFACy kGQAp sMi44c lNHeqe WGvvNb']\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(5)\n",
    "        picture=[imgage for imgage in driver.find_elements_by_xpath(\"//a[@class='VFACy kGQAp sMi44c lNHeqe WGvvNb']\")[:10]]\n",
    "        image_urls=[]\n",
    "        print(len(picture))\n",
    "        if(len(picture)==10):\n",
    "            for i in picture:\n",
    "                try:\n",
    "                    i.click()\n",
    "                    image_urls.append(i.get_attribute('href'))\n",
    "                except:\n",
    "                    image_urls.append('Not Available')\n",
    "            return image_urls\n",
    "        else:\n",
    "            print('Image Not Available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358710f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Let's scrape the url's of all images\n",
    "fruits=get_urls('Fruits')\n",
    "cars=get_urls('Cars')\n",
    "machine_learning=get_urls('Machine Learning')\n",
    "guitar=get_urls('Guitar')\n",
    "cakes=get_urls('Cakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4123f3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Cars</th>\n",
       "      <th>Machine Learning</th>\n",
       "      <th>Guitar</th>\n",
       "      <th>Cakes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "      <td>https://www.google.com/url?sa=i&amp;url=https%3A%2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Fruits  \\\n",
       "0                                      Not Available   \n",
       "1                                      Not Available   \n",
       "2                                      Not Available   \n",
       "3                                      Not Available   \n",
       "4  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "5  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "6  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "7  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "8  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "9  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "\n",
       "                                                Cars  \\\n",
       "0                                      Not Available   \n",
       "1                                      Not Available   \n",
       "2                                      Not Available   \n",
       "3                                      Not Available   \n",
       "4  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "5  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "6  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "7  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "8  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "9  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "\n",
       "                                    Machine Learning  \\\n",
       "0  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "1  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "2  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "3  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "4                                      Not Available   \n",
       "5  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "6  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "7  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "8  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "9  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "\n",
       "                                              Guitar  \\\n",
       "0  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "1  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "2  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "3  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "4  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "5  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "6  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "7  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "8  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "9  https://www.google.com/url?sa=i&url=https%3A%2...   \n",
       "\n",
       "                                               Cakes  \n",
       "0  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "1  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "2  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "3  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "4  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "5  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "6  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "7  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "8  https://www.google.com/url?sa=i&url=https%3A%2...  \n",
       "9  https://www.google.com/url?sa=i&url=https%3A%2...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a DataFrame for our data\n",
    "Image_URL=pd.DataFrame({})\n",
    "Image_URL['Fruits']=fruits\n",
    "Image_URL['Cars']=cars\n",
    "Image_URL['Machine Learning']=machine_learning\n",
    "Image_URL['Guitar']=guitar\n",
    "Image_URL['Cakes']=cakes\n",
    "Image_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a6ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890aca4",
   "metadata": {},
   "source": [
    "# 4.\n",
    "Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813eae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.flipkart.com/')\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88455ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's enter the details in the search column\n",
    "product=input('Enter the product vertival that needs to be searched : ')\n",
    "driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys(str(product))\n",
    "driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "driver.implicitly_wait(3)\n",
    "#smartphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edeffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create an empty list to store the scrape data\n",
    "product_urls=[]\n",
    "brand_name=[]\n",
    "smartphone_name=[]\n",
    "smartphone_color=[]\n",
    "RAM=[]\n",
    "ROM=[]\n",
    "primary_camera=[]\n",
    "secondary_camera=[]\n",
    "display_size=[]\n",
    "battery_capacity=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define a function\n",
    "links=driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "\n",
    "#Let's scrape product URL\n",
    "for i in links:\n",
    "    product_urls.append(i.get_attribute('href'))\n",
    "for i in product_urls:\n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(4)\n",
    "    driver.find_elements_by_xpath('//div[@class=\"_4rR01T\"]')\n",
    "    \n",
    "    try: \n",
    "#Let's scrape brand name\n",
    "        title=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]').text.split()\n",
    "        brand_name.append(title[0])\n",
    "    except:\n",
    "        brand_name.append('-')\n",
    "        \n",
    "    try:\n",
    "#Let's scrape smartohone name\n",
    "        smartphone_name.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li').text)\n",
    "    except:\n",
    "        smartphone_name.append('-')\n",
    "        \n",
    "    try:     \n",
    "#Let's scrape smartphone colour\n",
    "        smartphone_color.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li').text)\n",
    "    except:\n",
    "        smartphone_color.append('-')\n",
    "        \n",
    "    try:       \n",
    "#Let's scrape smartphone RAM\n",
    "        RAM.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li').text)\n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "        \n",
    "    try:      \n",
    "#Let's scrape smartphone ROM\n",
    "        ROM.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li').text)\n",
    "    except NoSuchElementException:\n",
    "        ROM.append('-')\n",
    "        \n",
    "    try:      \n",
    "#Let's scrape smartphone primary camera details\n",
    "        primary_camera.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li').text)\n",
    "    except NoSuchElementException:\n",
    "        primary_camera.append('-')\n",
    "        \n",
    "    try:     \n",
    "#Let's scrape smartphone secondary camera details\n",
    "        secondary_camera.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li').text)\n",
    "    except NoSuchElementException:\n",
    "        secondary_camera.append('-')\n",
    "        \n",
    "    try:    \n",
    "#Let's scrape smartphone display size\n",
    "        display_size.append(driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li').text)\n",
    "    except NoSuchElementException:\n",
    "        display_size.append('-')\n",
    "        \n",
    "    try:   \n",
    "#Let's scrape smartphone battery capacity\n",
    "        battery_capacity.append(driver.find_element_by_xpath('//div[@class=\"_2418kt\"]/ul/li[4]').text)\n",
    "    except:\n",
    "        battery_capacity.append('-')\n",
    "        \n",
    "    try:  \n",
    "#Let's scrape smartphone price\n",
    "        price.append(driver.find_element_by_xpath('//div[@class=\"dyC4hf\"]/div/div/div').text)\n",
    "    except:\n",
    "        price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart=pd.DataFrame({})\n",
    "flipkart['Brand Name']=brand_name\n",
    "flipkart['Smartphone Name']=smartphone_name\n",
    "flipkart['Smartphone Colour']=smartphone_color\n",
    "flipkart['RAM']=RAM\n",
    "flipkart['Storage (ROM)']=ROM\n",
    "flipkart['Primray Camera']=primary_camera\n",
    "flipkart['Secondary Camera']=secondary_camera\n",
    "flipkart['Display Size']=display_size\n",
    "flipkart['Battery Capacity']=battery_capacity\n",
    "flipkart['Price']=price\n",
    "flipkart['Product URL']=product_urls\n",
    "flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b05db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save the dataframe in csv file\n",
    "flipkart.to_csv('flipkart_smartphone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3ade1",
   "metadata": {},
   "source": [
    "# 5.\n",
    "Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60abfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.google.com/maps/')\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3ddae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city name: goa\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"city name: \")\n",
    "search_bar = driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")\n",
    "search_bar.send_keys(user_input)\n",
    "search_btn = driver.find_element_by_xpath(\"//div[@class='xoLGzf-BIqFsb-haAclf']\").click()\n",
    "#search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02da67a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/maps/place/Goa/@21.0508654,75.7952726,15z/data=!4m5!3m4!1s0x3bbfba106336b741:0xeaf887ff62f34092!8m2!3d15.2993265!4d74.123996'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = driver.current_url\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3af7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['21.0508654', '75.7952726']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "cord = re.findall(r'@(.*),',url)[0].split(\",\")\n",
    "cord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e6c905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-ordinates of goa are : \n",
      "Lattitude: 21.0508654\n",
      "Longitude: 75.7952726\n"
     ]
    }
   ],
   "source": [
    "print(\"Co-ordinates of\", user_input, \"are : \")\n",
    "print(\"Lattitude:\", cord[0])\n",
    "print(\"Longitude:\", cord[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e846a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9a2ed",
   "metadata": {},
   "source": [
    "# 6. \n",
    "Write a program to scrap details of all the funding deals for Jan 21 – March 21 from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d1b9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://trak.in/')\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d9f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_bar = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(menu_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e9a952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Startup-name</th>\n",
       "      <th>Industry/Vertical</th>\n",
       "      <th>Sub-vertical</th>\n",
       "      <th>City</th>\n",
       "      <th>Investor's Name</th>\n",
       "      <th>Investment Type</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/01/2021</td>\n",
       "      <td>Digit Insurance</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Insurance Services</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>A91 Partners, Faering Capital, TVS Capital Funds</td>\n",
       "      <td>Venture</td>\n",
       "      <td>1,80,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28/01/2021</td>\n",
       "      <td>Bombay Shaving Company</td>\n",
       "      <td>Consumer Goods Company</td>\n",
       "      <td>Shave care, beard care, and skincare products</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Reckitt Benckiser</td>\n",
       "      <td>Venture</td>\n",
       "      <td>6,172,258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>DeHaat</td>\n",
       "      <td>AgriTech Startup</td>\n",
       "      <td>online marketplace for farm products and services</td>\n",
       "      <td>Patna</td>\n",
       "      <td>Prosus Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>Darwinbox</td>\n",
       "      <td>SaaS</td>\n",
       "      <td>HR Tech</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Salesforce Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>mfine</td>\n",
       "      <td>Health Tech Startup</td>\n",
       "      <td>AI-powered telemedicine mobile app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Heritas Capital Management</td>\n",
       "      <td>Venture Round</td>\n",
       "      <td>16,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>Udayy</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Online learning platform for kids in class 1-5</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Sequoia Capital</td>\n",
       "      <td>Seed Funding</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11/01/2021</td>\n",
       "      <td>True Elements</td>\n",
       "      <td>Food Startup</td>\n",
       "      <td>Whole Food plant based Nashta</td>\n",
       "      <td>Pune</td>\n",
       "      <td>SIDBI Venture Capital</td>\n",
       "      <td>Series</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13/01/2021</td>\n",
       "      <td>Saveo</td>\n",
       "      <td>B2B E-commerce</td>\n",
       "      <td>Pharmacies</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Matrix Partners India, RTP Global, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11/02/2021</td>\n",
       "      <td>Doubtnut</td>\n",
       "      <td>Edu Tech</td>\n",
       "      <td>E-Learning Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SIG Global, Sequoia Capital, WaterBridge Ventu...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>2,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22/02/2021</td>\n",
       "      <td>Zomato</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Online Food Delivery Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Tiger Global, Kora</td>\n",
       "      <td>Venture</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19/02/2021</td>\n",
       "      <td>Fingerlix</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Semi-cooked food delivery app</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Rhodium Trust, Accel Partners and Swiggy</td>\n",
       "      <td>Series C</td>\n",
       "      <td>2,747,045.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17/02/2021</td>\n",
       "      <td>Zolve</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Global Neobank Venture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Accel Partners and Lightspeed Venture Partners</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,50,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15/02/2021</td>\n",
       "      <td>KreditBee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Digital lending platform</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Azim Premji’s PremjiInvest and South Korea’s M...</td>\n",
       "      <td>Series C</td>\n",
       "      <td>75,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Pepperfry</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Multi-brand furniture brand</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>InnoVen Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>4,773,958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Online supermarket</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SoftBank Vision Fund (SVF)</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>55,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Technology Venture</td>\n",
       "      <td>London</td>\n",
       "      <td>GV</td>\n",
       "      <td>Series A</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>SplashLearn</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Game-based learning programme</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Owl Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>18,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>04/03/2021</td>\n",
       "      <td>DealShare</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online shopping platform</td>\n",
       "      <td>Jaipur, Rajasthan</td>\n",
       "      <td>Innoven Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31/03/2021</td>\n",
       "      <td>Uniphore</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Conversational Service Automation (CSA)</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Sorenson Capital Partners</td>\n",
       "      <td>Series D</td>\n",
       "      <td>140,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Hyper-local delivery app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Krishtal Advisors Pte Ltd</td>\n",
       "      <td>Series E</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>BYJU’S</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Online tutoring</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>MC Global Edtech, B Capital, Baron, others</td>\n",
       "      <td>Series F</td>\n",
       "      <td>460,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23/03/2021</td>\n",
       "      <td>SkilloVilla</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Career and job-oriented upskilling.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Titan Capital, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>300,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>CityMall</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Social ecommerce and online grocery platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Accel Partners</td>\n",
       "      <td>Series A</td>\n",
       "      <td>11,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>DotPe</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Commerce and payments platform to offline ente...</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Series A</td>\n",
       "      <td>27,500,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dates            Startup-name       Industry/Vertical  \\\n",
       "0   15/01/2021         Digit Insurance      Financial Services   \n",
       "1   28/01/2021  Bombay Shaving Company  Consumer Goods Company   \n",
       "2   19/01/2021                  DeHaat        AgriTech Startup   \n",
       "3   19/01/2021               Darwinbox                    SaaS   \n",
       "4   18/01/2021                   mfine     Health Tech Startup   \n",
       "5   18/01/2021                   Udayy                  EdTech   \n",
       "6   11/01/2021           True Elements            Food Startup   \n",
       "7   13/01/2021                   Saveo          B2B E-commerce   \n",
       "8   11/02/2021                Doubtnut                Edu Tech   \n",
       "9   22/02/2021                  Zomato             Hospitality   \n",
       "10  19/02/2021               Fingerlix             Hospitality   \n",
       "11  17/02/2021                   Zolve                 FinTech   \n",
       "12  15/02/2021               KreditBee                 Finance   \n",
       "13  12/02/2021               Pepperfry              E-commerce   \n",
       "14  12/02/2021                 Grofers              E-Commerce   \n",
       "15  09/02/2021                 Nothing              Technology   \n",
       "16  09/02/2021             SplashLearn                  EdTech   \n",
       "17  04/03/2021               DealShare              E-commerce   \n",
       "18  31/03/2021                Uniphore              Technology   \n",
       "19  30/03/2021                   Dunzo              E-commerce   \n",
       "20  30/03/2021                  BYJU’S                Edu-tech   \n",
       "21  23/03/2021             SkilloVilla                Edu-tech   \n",
       "22  25/03/2021                CityMall              E-commerce   \n",
       "23  26/03/2021                   DotPe                 FinTech   \n",
       "\n",
       "                                         Sub-vertical               City  \\\n",
       "0                                  Insurance Services          Bengaluru   \n",
       "1       Shave care, beard care, and skincare products          New Delhi   \n",
       "2   online marketplace for farm products and services              Patna   \n",
       "3                                             HR Tech             Mumbai   \n",
       "4                  AI-powered telemedicine mobile app          Bengaluru   \n",
       "5      Online learning platform for kids in class 1-5            Gurgaon   \n",
       "6                       Whole Food plant based Nashta               Pune   \n",
       "7                                          Pharmacies          Bengaluru   \n",
       "8                                 E-Learning Platform            Gurgaon   \n",
       "9                       Online Food Delivery Platform            Gurgaon   \n",
       "10                      Semi-cooked food delivery app             Mumbai   \n",
       "11                             Global Neobank Venture             Mumbai   \n",
       "12                           Digital lending platform          Bengaluru   \n",
       "13                        Multi-brand furniture brand             Mumbai   \n",
       "14                                 Online supermarket            Gurgaon   \n",
       "15                        Consumer Technology Venture             London   \n",
       "16                      Game-based learning programme            Gurgaon   \n",
       "17                           Online shopping platform  Jaipur, Rajasthan   \n",
       "18            Conversational Service Automation (CSA)          Palo Alto   \n",
       "19                           Hyper-local delivery app          Bengaluru   \n",
       "20                                    Online tutoring          Bengaluru   \n",
       "21                Career and job-oriented upskilling.          Bengaluru   \n",
       "22       Social ecommerce and online grocery platform            Gurgaon   \n",
       "23  Commerce and payments platform to offline ente...            Gurgaon   \n",
       "\n",
       "                                      Investor's Name Investment Type  \\\n",
       "0    A91 Partners, Faering Capital, TVS Capital Funds         Venture   \n",
       "1                                   Reckitt Benckiser         Venture   \n",
       "2                                     Prosus Ventures        Series C   \n",
       "3                                 Salesforce Ventures            Seed   \n",
       "4                          Heritas Capital Management   Venture Round   \n",
       "5                                     Sequoia Capital    Seed Funding   \n",
       "6                               SIDBI Venture Capital          Series   \n",
       "7           Matrix Partners India, RTP Global, others            Seed   \n",
       "8   SIG Global, Sequoia Capital, WaterBridge Ventu...        Series B   \n",
       "9                                  Tiger Global, Kora         Venture   \n",
       "10           Rhodium Trust, Accel Partners and Swiggy        Series C   \n",
       "11     Accel Partners and Lightspeed Venture Partners            Seed   \n",
       "12  Azim Premji’s PremjiInvest and South Korea’s M...        Series C   \n",
       "13                                    InnoVen Capital  Debt Financing   \n",
       "14                         SoftBank Vision Fund (SVF)     Unspecified   \n",
       "15                                                 GV        Series A   \n",
       "16                                       Owl Ventures        Series C   \n",
       "17                                    Innoven Capital  Debt Financing   \n",
       "18                          Sorenson Capital Partners        Series D   \n",
       "19                          Krishtal Advisors Pte Ltd        Series E   \n",
       "20         MC Global Edtech, B Capital, Baron, others        Series F   \n",
       "21                              Titan Capital, others            Seed   \n",
       "22                                     Accel Partners        Series A   \n",
       "23                                               PayU        Series A   \n",
       "\n",
       "          Amount  \n",
       "0    1,80,00,000  \n",
       "1   6,172,258.50  \n",
       "2     30,000,000  \n",
       "3     15,000,000  \n",
       "4     16,000,000  \n",
       "5     15,000,000  \n",
       "6    100,000,000  \n",
       "7      4,000,000  \n",
       "8      2,500,000  \n",
       "9    250,000,000  \n",
       "10  2,747,045.20  \n",
       "11   1,50,00,000  \n",
       "12    75,000,000  \n",
       "13     4,773,958  \n",
       "14    55,000,000  \n",
       "15    15,000,000  \n",
       "16    18,000,000  \n",
       "17   250,000,000  \n",
       "18   140,000,000  \n",
       "19     8,000,000  \n",
       "20   460,000,000  \n",
       "21   300,000,000  \n",
       "22    11,000,000  \n",
       "23    27,500,000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty lists to store data from scraping\n",
    "dt=[]\n",
    "name=[]\n",
    "ind=[]\n",
    "sv=[]\n",
    "city=[]\n",
    "inv=[]\n",
    "invt=[]\n",
    "amount=[]\n",
    "\n",
    "# scraping the Jan-21 month data\n",
    "d= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-2']\")\n",
    "for i in d:\n",
    "    dt.append(i.text)    \n",
    "n= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-3']\")\n",
    "for i in n:\n",
    "    name.append(i.text)    \n",
    "x= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-4']\")\n",
    "for i in x:\n",
    "    ind.append(i.text)    \n",
    "s= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-5']\")\n",
    "for i in s:\n",
    "    sv.append(i.text)    \n",
    "c= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-6']\")\n",
    "for i in c:\n",
    "    city.append(i.text)    \n",
    "j= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-7']\")\n",
    "for i in j:\n",
    "    inv.append(i.text)    \n",
    "t= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-8']\")\n",
    "for i in t:\n",
    "    invt.append(i.text)    \n",
    "a= driver.find_elements_by_xpath(\"//table[@id='tablepress-54']//td[@class='column-9']\")\n",
    "for i in a:\n",
    "    amount.append(i.text)\n",
    "    \n",
    "# scraping the Feb-21 month data\n",
    "d= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-2']\")\n",
    "for i in d:\n",
    "    dt.append(i.text)    \n",
    "n= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-3']\")\n",
    "for i in n:\n",
    "    name.append(i.text)    \n",
    "v= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-4']\")\n",
    "for i in v:\n",
    "    ind.append(i.text)    \n",
    "s= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-5']\")\n",
    "for i in s:\n",
    "    sv.append(i.text)    \n",
    "c= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-6']\")\n",
    "for i in c:\n",
    "    city.append(i.text)    \n",
    "j= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-7']\")\n",
    "for i in j:\n",
    "    inv.append(i.text)    \n",
    "t= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-8']\")\n",
    "for i in t:\n",
    "    invt.append(i.text)    \n",
    "a= driver.find_elements_by_xpath(\"//table[@id='tablepress-55']//td[@class='column-9']\")\n",
    "for i in a:\n",
    "    amount.append(i.text)\n",
    "    \n",
    "# scraping the Mar-21 month data\n",
    "d= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-2']\")\n",
    "for i in d:\n",
    "    dt.append(i.text)    \n",
    "n= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-3']\")\n",
    "for i in n:\n",
    "    name.append(i.text)    \n",
    "v= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-4']\")\n",
    "for i in v:\n",
    "    ind.append(i.text)    \n",
    "s= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-5']\")\n",
    "for i in s:\n",
    "    sv.append(i.text)    \n",
    "c= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-6']\")\n",
    "for i in c:\n",
    "    city.append(i.text)    \n",
    "j= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-7']\")\n",
    "for i in j:\n",
    "    inv.append(i.text)    \n",
    "t= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-8']\")\n",
    "for i in t:\n",
    "    invt.append(i.text)    \n",
    "a= driver.find_elements_by_xpath(\"//table[@id='tablepress-56']//td[@class='column-9']\")\n",
    "for i in a:\n",
    "    amount.append(i.text)    \n",
    "\n",
    "# now creating a dataframe from all the collected information\n",
    "data = list(zip(dt,name,ind,sv,city,inv,invt,amount))\n",
    "df = pd.DataFrame(data, columns = [\"Dates\",\"Startup-name\",\"Industry/Vertical\",\"Sub-vertical\",\"City\",\n",
    "                                   \"Investor's Name\",\"Investment Type\",\"Amount\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "605b7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711d4cb",
   "metadata": {},
   "source": [
    "# 7. \n",
    "Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbee90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=(\"https://www.digit.in/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)\n",
    "#Let's navigate to menubar and click on top10\n",
    "top_10=driver.find_element_by_xpath(\"//div[@class='menu']/ul/li[3]/a\").click()\n",
    "#clicking on best gaming laptops option\n",
    "laptops=driver.find_element_by_xpath(\"//ul[@class='list-unstyled sidebar-list']/li[10]/a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2825a5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Campany Name</th>\n",
       "      <th>OS</th>\n",
       "      <th>Screen Display Size</th>\n",
       "      <th>HDD</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Dimension (mm)</th>\n",
       "      <th>Graphical Processor</th>\n",
       "      <th>Price (Rs.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACER NITRO 5</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>1 TB HDD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>AMD Ryzen 9 Octa Core | 2.4 GHz</td>\n",
       "      <td>2.4</td>\n",
       "      <td>363.4 x 255 x 23.9</td>\n",
       "      <td>NVIDIA GeForce RTX 3070</td>\n",
       "      <td>₹ 129,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI STEALTH 15M</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>Intel Core i7 11th Gen - 11375H | NA</td>\n",
       "      <td>1.7</td>\n",
       "      <td>358.3 x 248 x 16.15</td>\n",
       "      <td>NVIDIA GeForce RTX 3060</td>\n",
       "      <td>₹ 134,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (2560 X 1440)</td>\n",
       "      <td>2 TB SSD</td>\n",
       "      <td>32 GBGB DDR4</td>\n",
       "      <td>AMD Ryzen 9 Octa Core - 5900HX | 3.3 GHz</td>\n",
       "      <td>2.30</td>\n",
       "      <td>354 x 259 x 22.6</td>\n",
       "      <td>NVIDIA GeForce RTX 3080</td>\n",
       "      <td>₹ 193,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALIENWARE AREA 51M R2</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>17.3\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>10th Gen Intel® Core™ i7-10700 | 2.90 GHz</td>\n",
       "      <td>4.1</td>\n",
       "      <td>27.65 x 402.6 x 319.14</td>\n",
       "      <td>Intel® UHD Graphics 630</td>\n",
       "      <td>₹ 342,989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALIENWARE M15 R3</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (3840 X 2160)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>10th Gen Intel® Core™ i9-10980HK | NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>₹ 319,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>AMD Ryzen™ 9 5900HX | 3.3 GHz</td>\n",
       "      <td>2.30</td>\n",
       "      <td>35.4 x 25.9 x 2.26</td>\n",
       "      <td>NVIDIA® GeForce RTX™ 3070</td>\n",
       "      <td>₹ 215,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS ZEPHYRUS G14</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>14\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>AMD 3rd Gen Ryzen 9 | 3.3 GHz</td>\n",
       "      <td>1.65</td>\n",
       "      <td>32.5 x 22.1 x 1.8</td>\n",
       "      <td>NVIDIA GeForce RTX 2060</td>\n",
       "      <td>₹ 144,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LENOVO LEGION 5I</td>\n",
       "      <td>WINDOWS 10 PRO</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>10th Gen Intel® Core™ i5-10300H | 2.50 GHz</td>\n",
       "      <td>2.3</td>\n",
       "      <td>363.06 x 259.61 x 23.57</td>\n",
       "      <td>NVIDIA® GeForce® GTX 1650 4GB</td>\n",
       "      <td>₹ 76,988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS ROG ZEPHYRUS DUO 15</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (3840 X 1100)</td>\n",
       "      <td>512 GB SSD</td>\n",
       "      <td>4 GBGB DDR4</td>\n",
       "      <td>Intel Core i7 10th Gen 10875H | NA</td>\n",
       "      <td>2.4</td>\n",
       "      <td>268.30 x 360.00 x 20.90</td>\n",
       "      <td>NVIDIA GeForce RTX 2070 Max-Q</td>\n",
       "      <td>₹ 185,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACER ASPIRE 7 GAMING LAPTOP</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>512 GB SSD</td>\n",
       "      <td>8 GBGB DDR4</td>\n",
       "      <td>AMD Ryzen™ 5-5500U hexa-core | NA</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.29 x 36.3 x 25.4</td>\n",
       "      <td>NVIDIA® GeForce® GTX 1650</td>\n",
       "      <td>₹ 53,490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Campany Name               OS  Screen Display Size  \\\n",
       "0                 ACER NITRO 5       WINDOWS 10  15.6\" (1920 X 1080)   \n",
       "1              MSI STEALTH 15M       WINDOWS 10  15.6\" (1920 X 1080)   \n",
       "2       ASUS ROG STRIX SCAR 15       WINDOWS 10  15.6\" (2560 X 1440)   \n",
       "3        ALIENWARE AREA 51M R2  WINDOWS 10 HOME  17.3\" (1920 X 1080)   \n",
       "4             ALIENWARE M15 R3  WINDOWS 10 HOME  15.6\" (3840 X 2160)   \n",
       "5       ASUS ROG STRIX SCAR 15  WINDOWS 10 HOME  15.6\" (1920 X 1080)   \n",
       "6            ASUS ZEPHYRUS G14  WINDOWS 10 HOME    14\" (1920 X 1080)   \n",
       "7             LENOVO LEGION 5I   WINDOWS 10 PRO  15.6\" (1920 X 1080)   \n",
       "8     ASUS ROG ZEPHYRUS DUO 15       WINDOWS 10  15.6\" (3840 X 1100)   \n",
       "9  ACER ASPIRE 7 GAMING LAPTOP  WINDOWS 10 HOME  15.6\" (1920 X 1080)   \n",
       "\n",
       "          HDD           RAM                                   Processor  \\\n",
       "0    1 TB HDD  16 GBGB DDR4             AMD Ryzen 9 Octa Core | 2.4 GHz   \n",
       "1    1 TB SSD  16 GBGB DDR4        Intel Core i7 11th Gen - 11375H | NA   \n",
       "2    2 TB SSD  32 GBGB DDR4    AMD Ryzen 9 Octa Core - 5900HX | 3.3 GHz   \n",
       "3    1 TB SSD  16 GBGB DDR4   10th Gen Intel® Core™ i7-10700 | 2.90 GHz   \n",
       "4    1 TB SSD  16 GBGB DDR4       10th Gen Intel® Core™ i9-10980HK | NA   \n",
       "5    1 TB SSD  16 GBGB DDR4               AMD Ryzen™ 9 5900HX | 3.3 GHz   \n",
       "6    1 TB SSD  16 GBGB DDR4               AMD 3rd Gen Ryzen 9 | 3.3 GHz   \n",
       "7    1 TB SSD  16 GBGB DDR4  10th Gen Intel® Core™ i5-10300H | 2.50 GHz   \n",
       "8  512 GB SSD   4 GBGB DDR4          Intel Core i7 10th Gen 10875H | NA   \n",
       "9  512 GB SSD   8 GBGB DDR4           AMD Ryzen™ 5-5500U hexa-core | NA   \n",
       "\n",
       "  Weight           Dimension (mm)            Graphical Processor Price (Rs.)  \n",
       "0    2.4       363.4 x 255 x 23.9        NVIDIA GeForce RTX 3070   ₹ 129,990  \n",
       "1    1.7      358.3 x 248 x 16.15        NVIDIA GeForce RTX 3060   ₹ 134,990  \n",
       "2   2.30         354 x 259 x 22.6        NVIDIA GeForce RTX 3080   ₹ 193,990  \n",
       "3    4.1   27.65 x 402.6 x 319.14        Intel® UHD Graphics 630   ₹ 342,989  \n",
       "4     NA                       NA                             NA   ₹ 319,990  \n",
       "5   2.30       35.4 x 25.9 x 2.26      NVIDIA® GeForce RTX™ 3070   ₹ 215,990  \n",
       "6   1.65        32.5 x 22.1 x 1.8        NVIDIA GeForce RTX 2060   ₹ 144,990  \n",
       "7    2.3  363.06 x 259.61 x 23.57  NVIDIA® GeForce® GTX 1650 4GB    ₹ 76,988  \n",
       "8    2.4  268.30 x 360.00 x 20.90  NVIDIA GeForce RTX 2070 Max-Q   ₹ 185,000  \n",
       "9   2.15       2.29 x 36.3 x 25.4      NVIDIA® GeForce® GTX 1650    ₹ 53,490  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's create an empty list to store data\n",
    "name = []\n",
    "OS = []\n",
    "display = []\n",
    "processor = []\n",
    "HDD = []\n",
    "price = []\n",
    "RAM = []\n",
    "dimension = []\n",
    "GPU = []\n",
    "weight = []\n",
    "\n",
    "#Let's scrape the Company Name\n",
    "Name=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in Name:\n",
    "    name.append(i.text)\n",
    "\n",
    "#Let's scrape the OS type\n",
    "os=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    OS.append(i.text)\n",
    "\n",
    "#Let's scrape the display size\n",
    "Display=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in Display:\n",
    "    display.append(i.text)\n",
    "    \n",
    "#Let's scrape the dimensions\n",
    "size=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[8]/td[3]\")\n",
    "for i in size:\n",
    "    dimension.append(i.text)\n",
    "    \n",
    "#Let's scrape the processor type\n",
    "Processor=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[5]/td[3]\")\n",
    "for i in Processor:\n",
    "    processor.append(i.text)\n",
    "\n",
    "#Let's scrape the weight\n",
    "Weight=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[1]\")\n",
    "Weight_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[7]/td[3]\")\n",
    "for i in range(len(Weight)):\n",
    "        if Weight[i].text=='Weight':\n",
    "            weight.append(Weight_spec[i].text)\n",
    "            \n",
    "GPUs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr[9]/td[3]\")\n",
    "for i in GPUs:\n",
    "    GPU.append(i.text)\n",
    "    \n",
    "#Let's scrape the Memory\n",
    "Memory=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[1]\")\n",
    "Memory_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\") \n",
    "for i in range(len(Memory)):\n",
    "        if Memory[i].text=='Memory':\n",
    "            HDD.append(Memory_spec[i].text.split('/')[0])\n",
    "            RAM.append(Memory_spec[i].text.split('/')[1])\n",
    "        else:\n",
    "            HDD.append('No details available')\n",
    "            RAM.append('No details available')\n",
    "\n",
    "            \n",
    "price_spec=driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "for i in price_spec:\n",
    "    price.append(i.text)\n",
    "    \n",
    "gaming_laptop=pd.DataFrame({\"Campany Name\":name,\"OS\":OS,\"Screen Display Size\":display,\n",
    "                \"HDD\":HDD,\"RAM\":RAM,\"Processor\":processor,\n",
    "                \"Weight\":weight,\"Dimension (mm)\":dimension,\n",
    "                \"Graphical Processor\":GPU,\"Price (Rs.)\":price})\n",
    "gaming_laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "517f4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d96fe",
   "metadata": {},
   "source": [
    "# 8. \n",
    "Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2be8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "# obtaining the web page with given url\n",
    "url = 'https://www.forbes.com/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking on the option on the left side of the page\n",
    "optn = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "optn.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# now clicking on the 'Billionaires' option\n",
    "bill = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "bill.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# finally clicking on the 'World Billionaire' option after the mouse hover\n",
    "world_bill = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "world_bill.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a11acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create an empty list to store data\n",
    "Rank=[]\n",
    "Name=[]\n",
    "NetWorth=[]\n",
    "Age=[]\n",
    "Citizenship=[]\n",
    "Source=[]\n",
    "Industry=[]\n",
    "\n",
    "#Let's scrape the name\n",
    "name_=driver.find_elements_by_xpath(\"//div[@class='personName']\")\n",
    "for i in name_:\n",
    "    Name.append(i.text)\n",
    "\n",
    "#Let's scrape the rank\n",
    "rank_=driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "for i in rank_:\n",
    "    Rank.append(i.text)\n",
    "\n",
    "#Let's scrape the networth\n",
    "net_worth = driver.find_elements_by_xpath(\"//div[@class='netWorth']\")\n",
    "for i in net_worth:\n",
    "    NetWorth.append(i.text)\n",
    "#Let's scrape the age \n",
    "try:\n",
    "    age_= driver.find_elements_by_xpath(\"//div[@class='age']\")\n",
    "    for i in age_:\n",
    "        Age.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Age.append('-')\n",
    "#Let's scrape the citizenship\n",
    "try:\n",
    "    citizenship_=driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for i in citizenship_:\n",
    "        Citizenship.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Citizenship.append('-')\n",
    "#Let's scrape the source    \n",
    "try:\n",
    "    source_=driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for i in source_:\n",
    "        Source.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Source.append('-')\n",
    "#Let's scrape the industry    \n",
    "try:\n",
    "    industry_=driver.find_elements_by_xpath(\"//div[@class='category']\")\n",
    "    for i in industry_:\n",
    "        Industry.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Industry.append('-')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd1ffef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>NetWorth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>real estate</td>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russia</td>\n",
       "      <td>oil</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Israel</td>\n",
       "      <td>real estate, shipping</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Snapchat</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>banking</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank  Name  NetWorth  Age    Citizenship                 Source  \\\n",
       "0     NaN   NaN       NaN  NaN  United States                 Amazon   \n",
       "1     NaN   NaN       NaN  NaN  United States          Tesla, SpaceX   \n",
       "2     NaN   NaN       NaN  NaN         France                   LVMH   \n",
       "3     NaN   NaN       NaN  NaN  United States              Microsoft   \n",
       "4     NaN   NaN       NaN  NaN  United States               Facebook   \n",
       "..    ...   ...       ...  ...            ...                    ...   \n",
       "195   NaN   NaN       NaN  NaN      Australia            real estate   \n",
       "196   NaN   NaN       NaN  NaN         Russia                    oil   \n",
       "197   NaN   NaN       NaN  NaN         Israel  real estate, shipping   \n",
       "198   NaN   NaN       NaN  NaN  United States               Snapchat   \n",
       "199   NaN   NaN       NaN  NaN       Colombia                banking   \n",
       "\n",
       "                  Industry  \n",
       "0               Technology  \n",
       "1               Automotive  \n",
       "2         Fashion & Retail  \n",
       "3               Technology  \n",
       "4               Technology  \n",
       "..                     ...  \n",
       "195            Real Estate  \n",
       "196                 Energy  \n",
       "197            Diversified  \n",
       "198             Technology  \n",
       "199  Finance & Investments  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now creating a dataframe from all the collected information\n",
    "Billionaire=pd.DataFrame({})\n",
    "Billionaire['Rank']=Rank\n",
    "Billionaire['Name']=Name\n",
    "Billionaire['NetWorth']=NetWorth[:200]\n",
    "Billionaire['Age']=Age\n",
    "Billionaire['Citizenship']=Citizenship\n",
    "Billionaire['Source']=Source\n",
    "Billionaire['Industry']=Industry\n",
    "\n",
    "Billionaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcade0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09adcf3",
   "metadata": {},
   "source": [
    "# 9. \n",
    "Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "928432f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=(\"https://www.youtube.com/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86bcb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the search option on our web driver\n",
    "search_bar = driver.find_element_by_xpath(\"/html/body/ytd-app/div/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/form/div[1]/div[1]/input\")\n",
    "search_bar.send_keys('Why Russia is Invading Ukraine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91cb9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the search button\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/ytd-app/div/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/button/yt-icon\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "294d46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on the first video\n",
    "video= driver.find_element_by_xpath(\"/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-video-renderer[1]/div[1]/div/div[1]/div/h3/a/yt-formatted-string\")\n",
    "video.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "844fe8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create an empty list to store data\n",
    "Comment=[]\n",
    "Upvote=[]\n",
    "Time=[]\n",
    "\n",
    "import time\n",
    "for i in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0, 1000)\",\" \")\n",
    "\n",
    "# scraping comments\n",
    "try:\n",
    "    comment_=driver.find_elements_by_xpath(\"//yt-formatted-string[@id='content-text']\")\n",
    "    for i in comment_:\n",
    "        Comment.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Comment.append('-')\n",
    "    \n",
    "# scraping upvotes\n",
    "\n",
    "try:\n",
    "    upvote_=driver.find_elements_by_xpath(\"//span[@id='vote-count-middle']\")\n",
    "    for i in upvote_:\n",
    "        Upvote.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Upvote.append('-')\n",
    "    \n",
    "# Scraping time of the comment uploaded\n",
    "try:\n",
    "    time_ = driver.find_elements_by_xpath(\"//yt-formatted-string[@class='published-time-text above-comment style-scope ytd-comment-renderer']\")\n",
    "    for i in time_:\n",
    "        Time.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    times.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9472319d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>UpVote</th>\n",
       "      <th>Time Of Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you want to support Ukrainian creators on t...</td>\n",
       "      <td>1.1K</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so vital to see analysis of current event...</td>\n",
       "      <td>6.7K</td>\n",
       "      <td>3 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear real life lore.\\nI’m a Ukrainian and I di...</td>\n",
       "      <td>938</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm glad to see the energy aspect of this inva...</td>\n",
       "      <td>985</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I knew about the warm water port at Sevastopol...</td>\n",
       "      <td>303</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wow, this was just amazingly detailed, structu...</td>\n",
       "      <td>121</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As many others have, I would like to thank you...</td>\n",
       "      <td>71</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WHY can't creators like THIS GUY be our news p...</td>\n",
       "      <td>14K</td>\n",
       "      <td>6 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Someone in the comments section quoted Noam Ch...</td>\n",
       "      <td>28</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is very well researched documentary! Sad ...</td>\n",
       "      <td>37</td>\n",
       "      <td>20 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>It’s a great video, and very helpful to unders...</td>\n",
       "      <td>19</td>\n",
       "      <td>1 day ago (edited)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Great information and analysis</td>\n",
       "      <td>44</td>\n",
       "      <td>2 days ago (edited)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thank you for providing history on this becaus...</td>\n",
       "      <td>30</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\" The general population doesn't know what's r...</td>\n",
       "      <td>3.1K</td>\n",
       "      <td>4 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Why can’t the news be more like this guy like ...</td>\n",
       "      <td>95</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thank you so much for this. I hope you recogni...</td>\n",
       "      <td>6</td>\n",
       "      <td>20 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>No one in the world has explain so clearly as ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Very well detailed video and really well prese...</td>\n",
       "      <td>52</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hey @RealLifeLore, I am so happy you got refer...</td>\n",
       "      <td>4</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This is a very helpful video on understanding ...</td>\n",
       "      <td>27K</td>\n",
       "      <td>8 days ago (edited)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>So basically - the Cold War never stopped! It ...</td>\n",
       "      <td>2.2K</td>\n",
       "      <td>5 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Very impressed with this video! It’s refreshin...</td>\n",
       "      <td>171</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Thank you for the analysis.  I highly recommen...</td>\n",
       "      <td>197</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>So true how they always say \"history repeats i...</td>\n",
       "      <td>5</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Thank you. I was wondering why this was happen...</td>\n",
       "      <td>197</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hello ! Thank you for these explanations and a...</td>\n",
       "      <td>2</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bro you've explained more in 30 minutes than h...</td>\n",
       "      <td>3.3K</td>\n",
       "      <td>6 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Excellent content. Can't stress enough how eff...</td>\n",
       "      <td>2</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Thank you for this! I’ve been trying to find i...</td>\n",
       "      <td>95</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Wow.  Within the first 10 minutes of your vide...</td>\n",
       "      <td>2</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>This in-depth analysis is a breath of fresh ai...</td>\n",
       "      <td>91</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I appreciate the unbiased, factual approach to...</td>\n",
       "      <td>29</td>\n",
       "      <td>19 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Thank god for this. I’ve been following the cu...</td>\n",
       "      <td>4K</td>\n",
       "      <td>8 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Thank you for this objective, informative, and...</td>\n",
       "      <td>91</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Damn! I Appreciate your diligence in the matte...</td>\n",
       "      <td>2</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Thank you for this amazing video. It is so hel...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>This video is by far the most comprehensive an...</td>\n",
       "      <td></td>\n",
       "      <td>7 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Seriously, this is the best explanation I’ve h...</td>\n",
       "      <td>1</td>\n",
       "      <td>14 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>This video was really well researched, well ex...</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>7 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Thank you for such an insightful documentary, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Let's all pay respect for the Ukrainian citize...</td>\n",
       "      <td>1.2K</td>\n",
       "      <td>8 days ago (edited)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>You mentioned that Russia backed protests took...</td>\n",
       "      <td>2</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>This was an amazing video thank you! Glad you ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Thank you so much for making this video. It's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 hour ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Please make subtitles for as many languages as...</td>\n",
       "      <td>4</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>You have a new subscriber, because this was in...</td>\n",
       "      <td></td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>I appreciate how you are explaining the reason...</td>\n",
       "      <td>2.4K</td>\n",
       "      <td>8 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This was a great video that brought this terri...</td>\n",
       "      <td>7</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>This video misses another important aspect of ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Very informative! I was looking for a deeper l...</td>\n",
       "      <td>7</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>thank you so much for this video! i'm often ra...</td>\n",
       "      <td>6</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Very good video, makes understanding the motiv...</td>\n",
       "      <td></td>\n",
       "      <td>20 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Let’s take a moment to appreciate RLL and his ...</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>8 days ago (edited)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>I am very grateful for your providing this.  Y...</td>\n",
       "      <td>16</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Excellent and insightful. I've read a lot of W...</td>\n",
       "      <td>28</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>This is a fantastic and educational video! How...</td>\n",
       "      <td>1</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Thank you so much for making this educational ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Now I understand why, thank you  I don't blame...</td>\n",
       "      <td>2</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>This shows exactly why energy independence is ...</td>\n",
       "      <td>2.1K</td>\n",
       "      <td>5 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Thank you, we now know what the conflict is ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comment UpVote  \\\n",
       "0   If you want to support Ukrainian creators on t...   1.1K   \n",
       "1   It's so vital to see analysis of current event...   6.7K   \n",
       "2   Dear real life lore.\\nI’m a Ukrainian and I di...    938   \n",
       "3   I'm glad to see the energy aspect of this inva...    985   \n",
       "4   I knew about the warm water port at Sevastopol...    303   \n",
       "5   wow, this was just amazingly detailed, structu...    121   \n",
       "6   As many others have, I would like to thank you...     71   \n",
       "7   WHY can't creators like THIS GUY be our news p...    14K   \n",
       "8   Someone in the comments section quoted Noam Ch...     28   \n",
       "9   This is very well researched documentary! Sad ...     37   \n",
       "10  It’s a great video, and very helpful to unders...     19   \n",
       "11                    Great information and analysis      44   \n",
       "12  Thank you for providing history on this becaus...     30   \n",
       "13  \" The general population doesn't know what's r...   3.1K   \n",
       "14  Why can’t the news be more like this guy like ...     95   \n",
       "15  Thank you so much for this. I hope you recogni...      6   \n",
       "16  No one in the world has explain so clearly as ...      5   \n",
       "17  Very well detailed video and really well prese...     52   \n",
       "18  Hey @RealLifeLore, I am so happy you got refer...      4   \n",
       "19  This is a very helpful video on understanding ...    27K   \n",
       "20  So basically - the Cold War never stopped! It ...   2.2K   \n",
       "21  Very impressed with this video! It’s refreshin...    171   \n",
       "22  Thank you for the analysis.  I highly recommen...    197   \n",
       "23  So true how they always say \"history repeats i...      5   \n",
       "24  Thank you. I was wondering why this was happen...    197   \n",
       "25  Hello ! Thank you for these explanations and a...      2   \n",
       "26  Bro you've explained more in 30 minutes than h...   3.3K   \n",
       "27  Excellent content. Can't stress enough how eff...      2   \n",
       "28  Thank you for this! I’ve been trying to find i...     95   \n",
       "29  Wow.  Within the first 10 minutes of your vide...      2   \n",
       "30  This in-depth analysis is a breath of fresh ai...     91   \n",
       "31  I appreciate the unbiased, factual approach to...     29   \n",
       "32  Thank god for this. I’ve been following the cu...     4K   \n",
       "33  Thank you for this objective, informative, and...     91   \n",
       "34  Damn! I Appreciate your diligence in the matte...      2   \n",
       "35  Thank you for this amazing video. It is so hel...      1   \n",
       "36  This video is by far the most comprehensive an...          \n",
       "37  Seriously, this is the best explanation I’ve h...      1   \n",
       "38  This video was really well researched, well ex...   8.3K   \n",
       "39  Thank you for such an insightful documentary, ...      1   \n",
       "40  Let's all pay respect for the Ukrainian citize...   1.2K   \n",
       "41  You mentioned that Russia backed protests took...      2   \n",
       "42  This was an amazing video thank you! Glad you ...      3   \n",
       "43  Thank you so much for making this video. It's ...      1   \n",
       "44  Please make subtitles for as many languages as...      4   \n",
       "45  You have a new subscriber, because this was in...          \n",
       "46  I appreciate how you are explaining the reason...   2.4K   \n",
       "47  This was a great video that brought this terri...      7   \n",
       "48  This video misses another important aspect of ...      9   \n",
       "49  Very informative! I was looking for a deeper l...      7   \n",
       "50  thank you so much for this video! i'm often ra...      6   \n",
       "51  Very good video, makes understanding the motiv...          \n",
       "52  Let’s take a moment to appreciate RLL and his ...   1.4K   \n",
       "53  I am very grateful for your providing this.  Y...     16   \n",
       "54  Excellent and insightful. I've read a lot of W...     28   \n",
       "55  This is a fantastic and educational video! How...      1   \n",
       "56  Thank you so much for making this educational ...      2   \n",
       "57  Now I understand why, thank you  I don't blame...      2   \n",
       "58  This shows exactly why energy independence is ...   2.1K   \n",
       "59  Thank you, we now know what the conflict is ab...      1   \n",
       "\n",
       "        Time Of Comment  \n",
       "0            2 days ago  \n",
       "1            3 days ago  \n",
       "2             1 day ago  \n",
       "3            2 days ago  \n",
       "4             1 day ago  \n",
       "5            2 days ago  \n",
       "6             1 day ago  \n",
       "7            6 days ago  \n",
       "8             1 day ago  \n",
       "9          20 hours ago  \n",
       "10   1 day ago (edited)  \n",
       "11  2 days ago (edited)  \n",
       "12           2 days ago  \n",
       "13           4 days ago  \n",
       "14           2 days ago  \n",
       "15         20 hours ago  \n",
       "16            1 day ago  \n",
       "17           2 days ago  \n",
       "18            1 day ago  \n",
       "19  8 days ago (edited)  \n",
       "20           5 days ago  \n",
       "21           2 days ago  \n",
       "22           2 days ago  \n",
       "23            1 day ago  \n",
       "24           2 days ago  \n",
       "25            1 day ago  \n",
       "26           6 days ago  \n",
       "27           2 days ago  \n",
       "28           2 days ago  \n",
       "29          2 hours ago  \n",
       "30           2 days ago  \n",
       "31         19 hours ago  \n",
       "32           8 days ago  \n",
       "33           2 days ago  \n",
       "34            1 day ago  \n",
       "35            1 day ago  \n",
       "36          7 hours ago  \n",
       "37         14 hours ago  \n",
       "38           7 days ago  \n",
       "39            1 day ago  \n",
       "40  8 days ago (edited)  \n",
       "41            1 day ago  \n",
       "42           2 days ago  \n",
       "43           1 hour ago  \n",
       "44           2 days ago  \n",
       "45            1 day ago  \n",
       "46           8 days ago  \n",
       "47            1 day ago  \n",
       "48            1 day ago  \n",
       "49            1 day ago  \n",
       "50           2 days ago  \n",
       "51         20 hours ago  \n",
       "52  8 days ago (edited)  \n",
       "53           2 days ago  \n",
       "54           2 days ago  \n",
       "55          2 hours ago  \n",
       "56           2 days ago  \n",
       "57            1 day ago  \n",
       "58           5 days ago  \n",
       "59            1 day ago  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now creating a dataframe from all the collected information\n",
    "youtube=pd.DataFrame({})\n",
    "youtube['Comment']=Comment[:500]\n",
    "youtube['UpVote']=Upvote[:500]\n",
    "youtube['Time Of Comment']=Time[:500]\n",
    "\n",
    "youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f57bb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb8c7e",
   "metadata": {},
   "source": [
    "# 10. \n",
    "Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09c32d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oprning the chromedriver and the URL page\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=(\"https://www.hostelworld.com/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf1218cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select london\n",
    "driver.find_element_by_id('location-text-input-field').click()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath('//*[@id=\"search-input-field\"]').send_keys('London')\n",
    "time.sleep(4)\n",
    "\n",
    "# do click on search button\n",
    "driver.find_element_by_xpath('//*[@id=\"predicted-search-results\"]/li[2]').click()\n",
    "driver.find_element_by_id('search-button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e7e77a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hostel name</th>\n",
       "      <th>Distance from city centre</th>\n",
       "      <th>ratings</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Overall review</th>\n",
       "      <th>Privates from price</th>\n",
       "      <th>Dorms from price</th>\n",
       "      <th>Facilities</th>\n",
       "      <th>Property Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>St Christopher's Village</td>\n",
       "      <td>1.8km from city centre</td>\n",
       "      <td>8.2</td>\n",
       "      <td>11011</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>Rs5989</td>\n",
       "      <td>Rs1408.77 Rs1268</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>165 Borough High Street, London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generator London</td>\n",
       "      <td>3km from city centre</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6821</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Rs5989</td>\n",
       "      <td>Rs1408.77 Rs1268</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>Compton Place, Off 37 Tavistock Place, WC1, Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Safestay London Elephant &amp; Castle</td>\n",
       "      <td>1.7km from city centre</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4061</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>Rs5989</td>\n",
       "      <td>Rs1408.77 Rs1268</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>144-152 Walworth Road, London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smart Camden Inn Hostel</td>\n",
       "      <td>4.4km from city centre</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2756</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>Rs5989</td>\n",
       "      <td>Rs1408.77 Rs1268</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>55/57 Bayham Street, Camden, London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pickwick Hall</td>\n",
       "      <td>2.3km from city centre</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2286</td>\n",
       "      <td>Superb</td>\n",
       "      <td>Rs5989</td>\n",
       "      <td>Rs1408.77 Rs1268</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>7 Bedford Place, Bloomsbury, London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>TLK Apartments &amp; Hotel</td>\n",
       "      <td>Hotel - 19.9km from city centre</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>Rs7241</td>\n",
       "      <td>Rs1521</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>36-40 High Street, TLK Apartments &amp; Hotel, Lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Park Hotel</td>\n",
       "      <td>Hotel - 4.9km from city centre</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>Rs7241</td>\n",
       "      <td>Rs1521</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>64 Belgrave Road, Lillington and Longmoore Gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Hanover Hotel</td>\n",
       "      <td>Hotel - 2.1km from city centre</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>Rs7241</td>\n",
       "      <td>Rs1521</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>30 St. Georges Drive, Victoria, London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>London Waterloo Hostel</td>\n",
       "      <td>0.7km from city centre</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2457</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Rs7241</td>\n",
       "      <td>Rs1521</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>73 Lambeth Walk, London, London, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>The Walrus Hostel</td>\n",
       "      <td>0.2km from city centre</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3712</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>Rs7241</td>\n",
       "      <td>Rs1521</td>\n",
       "      <td>Free WiFi, Follows Covid-19 sanitation guidance</td>\n",
       "      <td>172 Westminster, Bridge Road, London, England</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Hostel name        Distance from city centre  \\\n",
       "0            St Christopher's Village           1.8km from city centre   \n",
       "1                    Generator London             3km from city centre   \n",
       "2   Safestay London Elephant & Castle           1.7km from city centre   \n",
       "3             Smart Camden Inn Hostel           4.4km from city centre   \n",
       "4                       Pickwick Hall           2.3km from city centre   \n",
       "..                                ...                              ...   \n",
       "75             TLK Apartments & Hotel  Hotel - 19.9km from city centre   \n",
       "76                         Park Hotel   Hotel - 4.9km from city centre   \n",
       "77                      Hanover Hotel   Hotel - 2.1km from city centre   \n",
       "78             London Waterloo Hostel           0.7km from city centre   \n",
       "79                  The Walrus Hostel           0.2km from city centre   \n",
       "\n",
       "   ratings Total reviews Overall review Privates from price  Dorms from price  \\\n",
       "0      8.2        11011        Fabulous              Rs5989  Rs1408.77 Rs1268   \n",
       "1      7.8         6821       Very Good              Rs5989  Rs1408.77 Rs1268   \n",
       "2      8.0         4061        Fabulous              Rs5989  Rs1408.77 Rs1268   \n",
       "3      8.5         2756        Fabulous              Rs5989  Rs1408.77 Rs1268   \n",
       "4      9.0         2286          Superb              Rs5989  Rs1408.77 Rs1268   \n",
       "..     ...           ...            ...                 ...               ...   \n",
       "75       -            0       No Rating              Rs7241            Rs1521   \n",
       "76       -            0       No Rating              Rs7241            Rs1521   \n",
       "77       -            0       No Rating              Rs7241            Rs1521   \n",
       "78     7.0         2457       Very Good              Rs7241            Rs1521   \n",
       "79     8.2         3712        Fabulous              Rs7241            Rs1521   \n",
       "\n",
       "                                         Facilities  \\\n",
       "0   Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "1   Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "2   Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "3   Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "4   Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "..                                              ...   \n",
       "75  Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "76  Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "77  Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "78  Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "79  Free WiFi, Follows Covid-19 sanitation guidance   \n",
       "\n",
       "                                 Property Description  \n",
       "0            165 Borough High Street, London, England  \n",
       "1   Compton Place, Off 37 Tavistock Place, WC1, Lo...  \n",
       "2              144-152 Walworth Road, London, England  \n",
       "3        55/57 Bayham Street, Camden, London, England  \n",
       "4        7 Bedford Place, Bloomsbury, London, England  \n",
       "..                                                ...  \n",
       "75  36-40 High Street, TLK Apartments & Hotel, Lon...  \n",
       "76  64 Belgrave Road, Lillington and Longmoore Gar...  \n",
       "77    30 St. Georges Drive, Victoria, London, England  \n",
       "78           73 Lambeth Walk, London, London, England  \n",
       "79      172 Westminster, Bridge Road, London, England  \n",
       "\n",
       "[80 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists to collect the required information\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []\n",
    "\n",
    "\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    # fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "    # fetching the distance from city centre    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        \n",
    "    # fetching the privates from price details\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "            \n",
    "    # fetching the dorms from price information\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            \n",
    "    # fetching all the facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "        \n",
    "    # fetching the urls of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # clicking on the show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    # fetching the ratings for the hostels\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "    # fetching the total reviews        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        \n",
    "    # fetching the overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "        \n",
    "    # fetching the property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "\n",
    "# now creating a dataframe from all the collected information\n",
    "data = list(zip(hostel_name,distance,rating,reviews,over_all,pvt_prices,dorms_price,facilities,description))       \n",
    "df = pd.DataFrame(data, columns = [\"Hostel name\",\"Distance from city centre\",\"ratings\",\"Total reviews\",\n",
    "                                   \"Overall review\",\"Privates from price\",\"Dorms from price\",\"Facilities\",\"Property Description\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66fb390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f7564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4673c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca05bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720500fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7504108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5664e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
